================================================================================
 DNABERT-2 for Structural Variant (SV) Detection and Validation
================================================================================

Project Version: 1.0
Author: [Shaker]
Date: 2025-09-16

--------------------------------------------------------------------------------
1. OVERVIEW
--------------------------------------------------------------------------------

This project provides an end-to-end bioinformatics pipeline to detect and validate structural variants (SVs) from long-read sequencing data. The core of the pipeline is a fine-tuned DNABERT-2 model, a state-of-the-art DNA language model.

The goal is not to discover SVs from scratch but to act as a sophisticated filter and classifier. It takes candidate SVs generated by standard callers, analyzes the genomic sequences around the variant, and assigns a confidence score, effectively distinguishing true SVs from false positives.

The workflow is as follows:
1.  Generate candidate SV calls from long-read data using multiple established tools (Sniffles2, pbsv, SVIM, cuteSV).
2.  Merge these candidate call sets into a high-confidence set using SURVIVOR.
3.  Preprocess the data by extracting reference and alternate DNA sequences for each candidate SV.
4.  Fine-tune the DNABERT-2 model on a "gold-standard" truth set (GIAB HG002) to learn the sequence patterns of true vs. false SVs.
5.  Use the trained model to predict and annotate a final VCF file with high-confidence scores.

--------------------------------------------------------------------------------
2. PROJECT STRUCTURE
--------------------------------------------------------------------------------

dnabert-sv/
│
├── data/
│   ├── reference.fasta         # Reference genome (must be placed here)
│   ├── reads.bam               # Aligned sequencing reads (must be placed here)
│   ├── giab_truth.vcf.gz       # Ground truth VCF for training (must be placed here)
│   └── candidate_vcfs/         # Directory for VCFs from SV callers
│
├── config.py                 # Central configuration for all file paths and parameters.
├── utils.py                  # Logging helper functions.
├── assembler.py              # Handles local de novo assembly for complex variants.
├── preprocess.py             # Script to prepare data for the model.
├── dataset.py                # PyTorch Dataset for loading and tokenizing data.
├── model.py                  # The DNABERT-2 model architecture with a classification head.
├── train.py                  # Core script for fine-tuning the model.
├── tune.py                   # (Optional) Script for hyperparameter optimization.
├── predict.py                # Script to apply the trained model to new data.
└── requirements.txt          # Python package dependencies.

--------------------------------------------------------------------------------
3. SETUP AND INSTALLATION
--------------------------------------------------------------------------------

**PREREQUISITES:**
- A Linux-based environment (WSL2 on Windows is recommended).
- Conda package manager (Miniconda is recommended).
- Git for cloning the repository.
- A powerful GPU with CUDA is STRONGLY recommended for training.

---
**STEP 1: Clone the Repository**
---
First, clone this project to your local machine.

```bash
git clone <your-repository-url>
cd dnabert-sv```

---
**STEP 2: Create and Activate the Conda Environment**
---
This is the most critical step. We will create a dedicated environment to manage all bioinformatics tools and Python libraries to prevent conflicts.

```bash
# Create the conda environment with all required tools
conda create -n sv-calling -c bioconda sniffles=2.3 pbsv=2.9.0 svim=2.1.0 cutesv=2.2.1 survivor=1.0.7 samtools python=3.9 -y

# Activate the newly created environment
conda activate sv-calling

# Install all the required Python packages
pip install -r requirements.txt```
**IMPORTANT:** You must run `conda activate sv-calling` every time you open a new terminal to work on this project.

---
**STEP 3: Download and Organize Data**
---
All data must be downloaded into the `dnabert-sv/data/` directory.

```bash
# Navigate to the data directory
cd data/

# Download Truth Set (GIAB HG002)
wget https://ftp.ncbi.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.bed
wget https://ftp.ncbi.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz
wget https://ftp.ncbi.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz.tbi

# Download Long Read BAM File
wget https://ftp.ncbi.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_HiFi-Revio_20231031/HG002_PacBio-HiFi-Revio_20231031_48x_GRCh38-GIABv3.bam
wget https://ftp.ncbi.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_HiFi-Revio_20231031/HG002_PacBio-HiFi-Revio_20231031_48x_GRCh38-GIABv3.bam.bai

# Download Reference Genome
wget https://ftp.ncbi.nih.gov/giab/ftp/release/references/GRCh38/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz
wget https://ftp.ncbi.nih.gov/giab/ftp/release/references/GRCh38/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz.fai
wget https://ftp.ncbi.nih.grov/giab/ftp/release/references/GRCh38/GRCh38_GIAB_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz.gzi

# Decompress the reference genome
gunzip GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz

# Navigate back to the project root directory
cd ..
STEP 4: Rename Files to Match Configuration
To ensure the scripts work out-of-the-box, rename the downloaded files to match the paths specified in config.py.
code
Bash
# Inside the data/ directory, run:
mv GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta reference.fasta
mv HG002_PacBio-HiFi-Revio_20231031_48x_GRCh38-GIABv3.bam reads.bam
# The rest of the file names should match config.py already.
Your data/ directory is now ready.
END-TO-END WORKFLOW
Follow these stages in order. Ensure your conda environment is active (conda activate sv-calling).
STAGE 1: Generate Candidate SV Calls
What it does: This stage uses four different state-of-the-art SV callers to analyze the long-read BAM file and produce four separate lists (VCF files) of potential structural variants. Using multiple callers provides a more comprehensive set of candidates.
Run the following commands from the dnabert-sv/ root directory.
code
Bash
# 1. Sniffles2
sniffles \
  --input data/reads.bam \
  --reference data/reference.fasta \
  --vcf data/candidate_vcfs/sniffles2.vcf \
  --threads 8 \
  --sample-id HG002 \
  --minsupport 5

# 2. pbsv
pbsv call \
  --hifi \
  data/reference.fasta \
  data/reads.bam \
  data/candidate_vcfs/pbsv.vcf

# 3. SVIM
svim alignment \
  data/candidate_vcfs/ \
  data/reads.bam \
  data/reference.fasta \
  --sample HG002 \
  --minimum_depth 5
# Rename SVIM's output for consistency
mv data/candidate_vcfs/variants/variants.vcf data/candidate_vcfs/svim.vcf

# 4. cuteSV
cuteSV \
  data/reads.bam \
  data/reference.fasta \
  data/candidate_vcfs/cutesv.vcf \
  data/candidate_vcfs/ \
  --min_support 5 \
  --sample HG002 \
  --threads 8
You should now have sniffles2.vcf, pbsv.vcf, svim.vcf, and cutesv.vcf in the data/candidate_vcfs/ directory.
STAGE 2: Preprocess Data for DNABERT-2
What it does: This is a crucial step that prepares the training data. The preprocess.py script will:
Load all candidate VCF files from data/candidate_vcfs/.
For each candidate SV, it checks if it matches a "true" SV from the GIAB truth set. This assigns a label (1 for true, 0 for false).
It extracts the DNA sequence from the reference genome around the SV (ref_seq).
It constructs the DNA sequence as it would appear with the variant (alt_seq).
It saves these sequence pairs and their labels into a single file (processed_sv_data.tsv).
code
Bash
python preprocess.py
This script may take a while to run. Upon completion, you will have data/processed_sv_data.tsv.
STAGE 3: Train the DNABERT-2 Model
What it does: This stage fine-tunes the DNABERT-2 model. The train.py script loads the processed_sv_data.tsv file and teaches the model to distinguish between the ref_seq and alt_seq pairs that correspond to true SVs versus those that are likely false positives. The trained model weights are saved to a file.
NOTE: This step is computationally intensive and will be very slow without a GPU.
code
Bash
python train.py
The script will print training and validation progress for each epoch. The best-performing model will be saved to outputs/dnabert_sv_classifier.pth.
STAGE 4 (Optional): Hyperparameter Tuning
What it does: For advanced users who want to squeeze out maximum performance, the tune.py script uses Ray Tune to automatically search for the best learning rate, batch size, and other hyperparameters. This is a time-consuming process.
code
Bash
python tune.py
The script will output the best hyperparameter combination found. You can then update these values in config.py and re-run Stage 3.
STAGE 5: Predict on New Data
What it does: This is the final step. The predict.py script loads your trained model and applies it to a VCF file of your choice. It processes each SV in the VCF, generates the ref/alt sequences, feeds them to the model, and annotates the VCF with the model's confidence score.
We will use the merged VCF file from our initial callers as the input for prediction.
code
Bash
# First, generate a list of the VCFs to merge
ls data/candidate_vcfs/*.vcf > data/candidate_vcfs/vcf_file_list.txt

# Merge them with SURVIVOR (requiring support from at least 2 callers)
SURVIVOR merge data/candidate_vcfs/vcf_file_list.txt 1000 2 1 1 0 data/candidate_vcfs/SURVIVOR_merged.vcf

# Now, run prediction on the high-confidence merged file
python predict.py --input_vcf data/candidate_vcfs/SURVIVOR_merged.vcf
Output:
The final result is a new VCF file located at outputs/predictions.vcf. This file is identical to the input VCF but contains an additional field in the INFO column for each SV: DNABERT_SCORE.
Example: DNABERT_SCORE=0.981 indicates the model is 98.1% confident that this is a true structural variant. You can use this score to filter your final call set with much higher precision.